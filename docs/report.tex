\documentclass{article} % For LaTeX2e
\usepackage{iclr2019_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{float}
\usepackage{csquotes}


\title{Learning Robust Representations by Projecting Superficial Statistics Out \\ ICLR 2019 Reproducibility Challenge}


\author{Marcin Witkowski \& Łukasz Klasiński \\
Institute of Computer Science \\
University of Wroclaw \\
Wroclaw, Poland \\
\texttt{\{288840,290043\}@uwr.edu.pl} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission. TODO:

\begin{document}

\maketitle

\begin{abstract}
The main goal of reproduced paper is to build a classifier that is not susceptible to covariance shift
of the data, thus helping neural network to create better domain generalization. They introduce new neural building
block - NGLCM and a method to project out textural information learned by CNN - HEX. In this
paper we tried to reproduce said method and compare results by training model on PACS \citep{Li2017dg}, MNIST and FERG-DB \citep{aneja2016modeling}.

\end{abstract}

\section{Method}

\subsection{NGLCM}

NGLCM is a neural block that mimics \textit{gray-level co-occurrence matrix} (GLCM) \citep{glcm}
idea. Main difference is than NGLCM can be trained like any other NN layer during back-propagation, allowing
to tune it's parameters. Thanks to that, this block can extract textural 
information about image but is not capable of extracting semantic information. This is used later to 
unlearn classifier layer from associating textual noises to labels. 

\subsection{HEX}

The main idea of HEX is to project predictions based on all information about data onto subspace which is orthogonal to the ones based on textual-only description of input.
Then we project-out unnecessary noise information. As a result we should obtain predictions which are more independent to covariance shifts, thus more reliable
in domain classification. It is achieved be using three different outputs:

\begin{align*}
F_A & = f([h(X;\theta), g(X;\phi)];\xi) \\
F_G & = f([\mathbf{0}, g(X;\phi);\xi]) \\
F_P & = f([h(X;\theta), \mathbf{0}];\xi)
\end{align*}

where $F_A$, $F_G$, $F_P$ stands respectively for the results from all representation, only textural representation and raw data.

Projecting $F_A$ onto the subspace that is orthogonal to $F_G$ with
$$F_L = (I - F_G(F_G^T F_G)^{-1} F_G^T) F_A$$
yields $F_L$ for parameter tuning. In testing time $F_P$ is used instead of $F_L$. More information about rationale of this method can be
found in appendix of original paper \citet{wang2018learning}

\pagebreak

\section{Reproduction details}
In this section we will describe how we tried to reproduce results featured in original paper.

\subsection{Implementation}

Everything was implemented using PyTorch, open-source machine learning library for Python. Since no code was provided with paper,
we had to write everything from scratch. We have performed all tests using Jupyter Notebooks so they are easy to repeat at will.

We have implemented NGLCM block correspondingly to the paper, although we had to guess what direction was used in 
GLCM matrix (we used default 0), as well as we had to guess MLP layer shape. Another assumption was that the output
of NGLCM is $16\times244$ matrix ($16\times16$ pushed through MLP layer), but we somehow had to make classifier vector. To accomplish that we `squashed`
it to $16\cdot244$ length vector to further concatenate it with AlexNet output and pass to HEX projection.

Same goes from HEX. We calculated $F_A$, $F_G$ and $F_P$ as in paper and finally coded $F_L$. Then results from $F_L$ 
are passed into final SoftMax classification layer.

\subsection{Datasets}

\subsubsection{MNIST}

Firstly, we generated 6 datasets where each of them has 100 images per label, 10k images total . Subsequent datasets have images rotated by angle
$\alpha \in \{0^{\circ}, 15^{\circ}, 30^{\circ}, 45^{\circ}, 60^{\circ}, 75^{\circ}\}$, each of them representing one domain.    
We use all domains but one for training and remaining one for testing.

\subsubsection{FERG-DB}

Like in original paper, we used modified FERG-DB with backgrounds correlated to emotions. As we did not have access to original dataset
used in paper, we generated set of datasets based on clean FERG database. While we used same partitioning of data ($50\%$ train, $30\%$ valid, $20\%$ test),
each of datasets has $\approx 10 000$ images instead of $50 000$. Samples of images from datasets ($\rho=0.8$) are presented below:

\begin{figure}[h]
    \minipage{0.48\textwidth}
    \includegraphics[width=\textwidth]{res/train.jpg}
    \caption*{(a) train set}
    \endminipage\hfill
    \minipage{0.288\textwidth}
    \includegraphics[width=\textwidth]{res/valid.jpg}
    \caption*{(b) validation set}
    \endminipage\hfill
    \minipage{0.192\textwidth}
    \includegraphics[width=\textwidth]{res/test.jpg}
    \caption*{(c) test set}
    \endminipage\hfill
\end{figure}

\subsubsection{PACS}

Similar to MNIST datasets, we divided images from PACS into 4 domains representing art, cartoon, photo and sketch styled
images. Then we used one domain for testing and remaining ones for train. We used all PACS data including `extra` ones.

\pagebreak

\subsection{Experiments}

During challenge we tried to reproduce three experiments described in original paper.

\subsubsection{MNIST}
    MNIST rotation dataset tests, where we tried to replicate results from paper, provided in Table 3. \citep{wang2018learning}. We used hyperparameters listed below:
        
        \begin{enumerate}
            \item optimizer: Adam
            \item learning rate: $10^{-3}$
            \item weight decay: $10^{-3}$
            \item 50 epochs
        \end{enumerate}

    Experiment went quite well and we managed to reach similar results as in original paper.

\subsubsection{FERG-DB}
    FERG-DB tests, where we prepared 10 different datasets, each created with background correlation variable $\rho \in\{0.0, 0.9\}$. Then we
    tested it using two models: HEX and modified HEX, where we replaced whole NGLCM module with single $224\times16$ MLP layer. In paper, testing using this modification is called `Ablation tests`.
        
        \begin{enumerate}
            \item optimizer: Adam
            \item learning rate: $5\cdot10^{-4}$
            \item 70-100 epochs
        \end{enumerate}

    Hex training was converging very quickly to $100\%$ validate accuracy so we shortened epochs to 70 (from 100 in paper) to save time.
    We also noticed that during Ablation test we sometimes had to rerun training because model over fitted to test
    data too quickly to reach good validation results. Also we were not sure what number of parameters to give for MLP layer,
    so we picked $224\times16$ since it is a bit less then number of parameters for NGLCM.

\subsubsection{PACS}
    PACS domain shift tests, where we wanted to replicate results, provided in Table 4. \citep{wang2018learning}. We used random data augmentation (crops and vertical flips) to prevent overfitting.
    We first trained AlexNet alone to see if we get similar results.
        
        \begin{enumerate}
            \item optimizer: Adam
            \item learning rate: $10^{-3}$
            \item weight decay: $0$ or $10^{-2}$
            \item 50 epochs
        \end{enumerate}

    We wanted do get the best results we could, so we used a two step learning. First we have loaded AlexNet pretrained
    on ImageNet dataset (provided by torch library) and trained only classifier. Then to fine tune the network we have added
    weight decay to optimizer and trained all parameters for additional 50 epochs. We saved model that got best results during
    this two stage training and calculated accuracy.

    \pagebreak

    After getting somehow positive results from AlexNet, we tested if HEXnet would perform better or similar. We used hyper parameters listed below:
        
        \begin{enumerate}
            \item optimizer: Adam
            \item learning rate: $10^{-5}$
            \item weight decay: $10^{-5}$
            \item 100 epochs
        \end{enumerate}

    Following the paper, we first trained 10 epochs of classifier layer of pretrained AlexNet, then plugged HEX in for
    additional 100 epochs. At first we had problems with really unstable gradients, so we had to turn learning rate down drastically in 
    comparison to AlexNet one. When we work it out, we didn't really get satisfying results no matter what strategy
    we used. At the end we came to conclusion that in original experiments special PACS learning heuristics, described in this paper \citep{Li2017dg} must have been used.
    Unfortunately we didn't have enough time to study and replicate such learning heuristics. Still we are surprised that
    under the same condition HEXnet performed far worse then AlexNet, which according to paper results should be worse.

\section{Results}
\subsection{MNIST}

\begin{table}[H]
    \centering
    \begin{tabular}{cccc}
        \hline
        Test domain & ADV & HEX & \color{blue} HEX \color{black} \\
        \hline
        $\mathcal{M}_{0^{\circ}}$    & 91.1 & 90.1 & 92.3 \\
        $\mathcal{M}_{{15}^{\circ}}$ & 98.2 & 98.9 & 98.7 \\
        $\mathcal{M}_{{30}^{\circ}}$ & 98.6 & 98.9 & 97.1 \\
        $\mathcal{M}_{{45}^{\circ}}$ & 98.7 & 98.8 & 97.9 \\
        $\mathcal{M}_{{60}^{\circ}}$ & 98.4 & 98.3 & 96.5 \\
        $\mathcal{M}_{{75}^{\circ}}$ & 92.0 & 90.0 & 93.1 \\
        \hline
        Avg & 96.2 & 95.8 & 95.9 \\
        \hline
    \end{tabular}
\end{table}

Our results are marked as blue. We managed to reproduce results obtained in original paper quite faithfully (Table 3  \citep{wang2018learning}).
\subsection{FERG-DB}

\begin{table}[H]
    \centering
    \begin{tabular}{ccc}
        \hline
        $\rho$ & HEX w/ MLP& HEX\\
        \hline
        $0.0$ & 98.9 & 99.4 \\
        $0.1$ & 99.2 & 99.6 \\
        $0.2$ & 98.8 & 99.7 \\
        $0.3$ & 98.2 & 99.4 \\
        $0.4$ & 99.0 & 99.1 \\
        $0.5$ & 99.0 & 99.5 \\
        $0.6$ & 98.0 & 99.5 \\
        $0.7$ & 87.2 & 98.4 \\
        $0.8$ & 91.4 & 98.4 \\
        $0.9$ & 49.2 & 91.6 \\
        \hline
        Avg & 91.9 & 98.5 \\
        \hline
    \end{tabular}
\end{table}


As we can see, HEX with NGLCM is much more stable in classification than bare CNN, yielding similar results
to those from paper (Figure 3.  \citep{wang2018learning}). 
Simple MLP layer isn't capable of removing background noise as good as NGLCM block.

\subsection{PACS}

\begin{table}[H]
    \centering
    \begin{tabular}{ccccc}
        \hline
        Test domain & AlexNet & \color{blue} AlexNet \color{black} & HEX & \color{blue} HEX \color{black} \\
        \hline
        Art     & 63.3 & 57.2 & 66.8 & 47.6 \\
        Cartoon & 63.1 & 61.3 & 69.7 & 65.0 \\
        Photo   & 87.7 & 81.3 & 87.9 & 53.3 \\
        Sketch  & 54.0 & 62.0 & 56.3 & 57.8 \\
        \hline
        Avg     & 67.0 & 65.5 & 70.2 & 55.9 \\
        \hline
    \end{tabular}
\end{table}

We realized too late that in paper they probably used different training heuristics from ours, so we have different results (Table 4 \citep{wang2018learning}).
Still we think it is strange that under the same conditions HEX performed worse then AlexNet.

\section{Conclusion}

During our \textquote{reproducibility challenge} we encountered some minor and major problems.
Paper did not contain most of informations about used hyper-parameters and some aspects of described
methods were omitted (ex. like used heuristics or how to vectorize output of NGLCM). Overall, we were
able to reproduce results of most of the experiments, besides PACS where we used different heuristic
for fine-tuning CNN.

\bibliography{iclr2019_conference}
\bibliographystyle{iclr2019_conference}

\end{document}
